{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import glob\n",
    "\n",
    "pdf_files = glob.glob(\"./articles/*.pdf\")\n",
    "\n",
    "loaders = [PyPDFLoader(file_path) for file_path in pdf_files]\n",
    "\n",
    "articles = []\n",
    "for loader in loaders:\n",
    "    article = loader.load()\n",
    "    articles.append(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\".join([page.page_content for page in articles[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_sources = set()\n",
    "metadata_lines = []\n",
    "\n",
    "for page in articles[0]:\n",
    "    source = page.metadata.get('source')\n",
    "    if source and source not in seen_sources:\n",
    "        metadata_lines.append(source)\n",
    "        seen_sources.add(source)\n",
    "\n",
    "metadata = \"\\n\".join(metadata_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    r\"^\\s*Abstract\\s*$\",\n",
    "    r\"^\\s*Introduction\\s*$\",\n",
    "    r\"^\\s*Methods\\s*$\",\n",
    "    r\"^\\s*Methodology\\s*$\",\n",
    "    r\"^\\s*Results\\s*$\",\n",
    "    r\"^\\s*Discussion\\s*$\",\n",
    "    r\"^\\s*Conclusion\\s*$\",\n",
    "    r\"^\\s*References\\s*$\"\n",
    "]\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_markdown(text, headers):\n",
    "    lines = text.split(\"\\n\")\n",
    "    markdown_text = []\n",
    "    \n",
    "    for line in lines:\n",
    "        header_found = False\n",
    "        for header in headers:\n",
    "            if re.match(header, line.strip(), re.IGNORECASE):\n",
    "                markdown_text.append(f\"# {line.strip()}\")\n",
    "                header_found = True\n",
    "                break\n",
    "        if not header_found:\n",
    "            markdown_text.append(line)\n",
    "    \n",
    "    return \"\\n\".join(markdown_text)\n",
    "\n",
    "# Convertendo o texto para formato Markdown\n",
    "markdown_text = convert_to_markdown(text, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo o texto Markdown usando MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_text)\n",
    "print(md_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in md_header_splits:\n",
    "    doc.metadata['source'] = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(md_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(md_header_splits)-1):\n",
    "    print(md_header_splits[index].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(md_header_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement small sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "embedding = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = './chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=md_header_splits,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory=persist_directory\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=\"./chroma\", embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Header\",\n",
    "        description=\"This is the header of the section from which this text originates\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"Path of the PDF file from which this chunk is derived. It is possible to observe the hash that symbolizes the name of the PDF, in addition to the number of questions it contains. The PDFs are medical scientific articles in the field of medicine. The path format is 'articles/hash(n).pdf', where 'hash' represents the PDF name, 'n' represents the number of related questions, and '.pdf' represents the file extension.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the main subject of the article?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(docs_ss)):\n",
    "    print(docs_ss[index].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(docs_ss)):\n",
    "    print(docs_ss[index].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_name = \"1f90a31355e180e376a2a4f420ca51970a772882\"\n",
    "\n",
    "# question = f'what did they say about Melanoma in the article \"{article_name}\"?'\n",
    "# docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are major topics for this article?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Is probability a class topic?\"\n",
    "# result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Is immunity a article topic?\"\n",
    "# result = qa_chain({\"query\": question})\n",
    "# result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"why are those prerequesites needed to understand this topic?\"\n",
    "# result = qa_chain({\"query\": question})\n",
    "# result['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyser test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\n",
    "quizz_question = \"\"\n",
    "possible_answers = \"\"\n",
    "answer_key = \"\"\n",
    "\n",
    "question = \"\"\"The following question is based on the article: {article}. \n",
    "The question statement, possible answers, and the correct answer are defined as follows:\n",
    "Question: {quizz_question}\n",
    "Possible answers: {possible_answers}\n",
    "Answer key: {answer_key}\n",
    "\n",
    "Please analyze the entire article {article} and determine as accurately as possible which section of the article this question pertains to. Also, justify your choice.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectordb.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa({\"query\": question}, return_only_outputs=True)\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_counts = {\n",
    "        \"Introduction\": 0,\n",
    "        \"Background\": 0,\n",
    "        \"Methods\": 0,\n",
    "        \"Results\": 0,\n",
    "        \"Discussion\": 0,\n",
    "        \"Conclusion\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in result['result']:\n",
    "        if \"Introduction\" in output or \"Background\" in output:\n",
    "            section_counts[\"Introduction\"] += 1\n",
    "            section_counts[\"Background\"] += 1\n",
    "        if \"Methods\" in output:\n",
    "            section_counts[\"Methods\"] += 1\n",
    "        if \"Results\" in output:\n",
    "            section_counts[\"Results\"] += 1\n",
    "        if \"Discussion\" in output:\n",
    "            section_counts[\"Discussion\"] += 1\n",
    "        if \"Conclusion\" in output:\n",
    "            section_counts[\"Conclusion\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(section_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Carregar o pipeline de classificação de texto\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Definir o texto e as categorias\n",
    "text = result['result']\n",
    "\n",
    "# Definir as seções como categorias\n",
    "labels = [\"Introduction\", \"Methods\", \"Results\", \"Discussion\", \"Conclusion\"]\n",
    "\n",
    "# Classificar o texto\n",
    "classification = classifier(text, labels)\n",
    "\n",
    "# Obter a seção mais provável\n",
    "predicted_section = classification['labels'][0]\n",
    "\n",
    "# Atualizar a contagem da seção\n",
    "section_counts = {\n",
    "    \"Introduction\": 0,\n",
    "    \"Methods\": 0,\n",
    "    \"Results\": 0,\n",
    "    \"Discussion\": 0,\n",
    "    \"Conclusion\": 0\n",
    "}\n",
    "\n",
    "section_counts[predicted_section] += 1\n",
    "\n",
    "# Mostrar a seção predita e as contagens atualizadas\n",
    "print(f\"Predicted section: {predicted_section}\")\n",
    "print(\"Section counts:\", section_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(questions_data, answers_data):\n",
    "    questions = []\n",
    "    \n",
    "    # Criar um dicionário para mapear id de perguntas para respostas\n",
    "    answer_map = {}\n",
    "    for answer in answers_data:\n",
    "        question_id = answer[\"question\"]\n",
    "        if question_id not in answer_map:\n",
    "            answer_map[question_id] = []\n",
    "        if float(answer[\"fraction\"]) > 0:\n",
    "            answer_map[question_id].append(answer[\"answer\"])\n",
    "    \n",
    "    # Criar lista de perguntas formatadas\n",
    "    for question in questions_data:\n",
    "        q_id = question[\"id\"]\n",
    "        article_id = \"1f90a31355e180e376a2a4f420ca51970a772882.pdf\"  # Assuming article ID remains constant\n",
    "        \n",
    "        quizz_question = question[\"questiontext\"]\n",
    "        \n",
    "        # Obter possíveis respostas para a pergunta\n",
    "        possible_answers = answer_map.get(q_id, [])\n",
    "        possible_answers_str = \";\".join(possible_answers)\n",
    "        \n",
    "        # Obter a chave de respostas corretas (answer_key)\n",
    "        answer_key = question.get(\"answer_key\", \"\")  # Assuming answer_key is provided in the questions_data\n",
    "        \n",
    "        # Montar o dicionário para a pergunta atual\n",
    "        question_dict = {\n",
    "            \"article\": article_id,\n",
    "            \"quizz_question\": quizz_question,\n",
    "            \"possible_answers\": possible_answers_str,\n",
    "            \"answer_key\": answer_key\n",
    "        }\n",
    "        \n",
    "        # Adicionar à lista de perguntas formatadas\n",
    "        questions.append(question_dict)\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Uso da função para transformar os dados\n",
    "questions = transform_data(questions_data, answers_data)\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_question = questions[5]\n",
    "question = ch_question['quizz_question']\n",
    "respostas_possiveis = ch_question['possible_answers']\n",
    "\n",
    "article = \"1f90a31355e180e376a2a4f420ca51970a772882.pdf\"\n",
    "quizz_question = ch_question['quizz_question']\n",
    "possible_answers = ch_question['possible_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "question = \"\"\"The following question is based on the article: {article}. \n",
    "The question statement, possible answers, and the correct answer are defined as follows:\n",
    "Question: {quizz_question}\n",
    "Possible answers: {possible_answers}\n",
    "\n",
    "Please analyze the entire article {article} and determine as accurately as possible which section of the article this question pertains to. Also, justify your choice.\"\"\"\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n",
    "\n",
    "retriever=vectordb.as_retriever()\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "\n",
    ")\n",
    "\n",
    "result = qa({\"query\": question}, return_only_outputs=True)\n",
    "result['result']\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Carregar o pipeline de classificação de texto\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Definir o texto e as categorias\n",
    "text = result['result']\n",
    "\n",
    "# Definir as seções como categorias\n",
    "labels = [\"Introduction\", \"Methods\", \"Results\", \"Discussion\", \"Conclusion\"]\n",
    "\n",
    "# Classificar o texto\n",
    "classification = classifier(text, labels)\n",
    "\n",
    "# Obter a seção mais provável\n",
    "predicted_section = classification['labels'][0]\n",
    "\n",
    "# Atualizar a contagem da seção\n",
    "section_counts = {\n",
    "    \"Introduction\": 0,\n",
    "    \"Methods\": 0,\n",
    "    \"Results\": 0,\n",
    "    \"Discussion\": 0,\n",
    "    \"Conclusion\": 0\n",
    "}\n",
    "\n",
    "section_counts[predicted_section] += 1\n",
    "\n",
    "# Mostrar a seção predita e as contagens atualizadas\n",
    "print(f\"Predicted section: {predicted_section}\")\n",
    "print(\"Section counts:\", section_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
